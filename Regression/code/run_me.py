# -*- coding: utf-8 -*-
"""ML_589_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q8xx9vZIFjIwzUXAk-KaZi574uxr3VvL

"""

import numpy as np
import warnings

warnings.filterwarnings("ignore")

stuff = np.load("data.npz")
X_trn = stuff["X_trn"]
y_trn = stuff["y_trn"]
X_val = stuff["X_val"]
y_val = stuff["y_val"]

# @title Helper functions
def error_cal(y_pred, y_true, method="abs"):
    if method == "abs":
        return abs(np.subtract(y_pred, y_true)).mean()
    else:
        return np.square(np.subtract(y_pred, y_true)).mean()


"""
#sandbox printing tests
YOUR_LIST = [1,2,3,4]
import math
print("|{:^10}|{:^10}|{:^10}|{:^10}|".format("test 1","test 2","test 3","test 4"))
print("={:=^11}{:=^11}{:=^11}{:=^11}".format("", "","",""))
for x in YOUR_LIST:
    print("|{:^10.2f}|{:^10.2f}|{:^10.2f}|{:^10.2f}|".format(x,1*x,1.5*(x),3*(x)))
    print("-{:-^11}{:-^11}{:-^11}{:-^11}".format("", "","",""))
"""


def print_helper(headers, rows):
    n_cols = len(headers)
    header_str_prefix = "|"
    header_str_rep = "{:^14}|" * n_cols

    header_row_sep_prefix = "="
    header_row_sep_rep = "{:=^15}" * n_cols

    row_str_prefix = "|"
    row_str_rep = "{:^14}|" * n_cols

    row_str_sep_prefix = "-"
    row_str_sep_rep = "{:-^15}" * n_cols

    final_ret = ""
    header_row = (header_str_prefix + header_str_rep).format(*headers)
    header_row_sep = (header_row_sep_prefix + header_row_sep_rep).format(
        *["" for i in range(n_cols)]
    )
    final_ret = header_row_sep + "\n" + header_row + "\n" + header_row_sep + "\n"
    for row in rows:
        row_row = (row_str_prefix + row_str_rep).format(*row)
        row_sep_row = (row_str_sep_prefix + row_str_sep_rep).format(
            *["" for i in range(n_cols)]
        )
        final_ret = final_ret + row_row + "\n" + row_sep_row + "\n"
    return final_ret


""" Write a method to do K-nearest neighbors regression. 
where X_trn is a 2D array of training inputs, y_trn is a 1D array of training outputs, 
x is a 1D array of a single input to make predictions for, and K is the number of neighbors. 
The return value y is just a scalar. """


def KNN_reg_predict(X_trn, y_trn, x, K):
    neighbor_list = []
    if len(X_trn) == len(y_trn):
        for (p, y_val) in zip(X_trn, y_trn):
            dist = np.linalg.norm(np.array(p) - np.array(x))
            neighbor_list.append((dist, y_val))
        neighbor_list[:] = sorted(neighbor_list, key=lambda tuple_: tuple_[0])[:K]
        y = sum(val[1] for val in neighbor_list) / K
        return y
    else:
        print("Number of points and outputs don't match.")


"""Question 2: where X_trn is a 2D array of training inputs, y_trn is a 1D array of training outputs, x is a 1D array of a single input to make predictions for, and K is the number of neighbors. The return value y is just a scalar."""

k = range(1, 11)


def KNN_predict(X_trn, y_trn, X_val, y_val, param, method):
    output = [KNN_reg_predict(X_trn, y_trn, inp, param) for inp in X_val]
    loss = error_cal(output, y_val, method)
    return loss


def KNN_error(X_trn, y_trn, X_val, y_val, params, method):
    errors = []
    for param in params:
        train_error = KNN_predict(X_trn, y_trn, X_trn, y_trn, param, method)
        test_error = KNN_predict(X_trn, y_trn, X_val, y_val, param, method)
        errors.append([param, round(train_error, 5), round(test_error, 5)])
    return errors


print("Q2:")

MSE_error = KNN_error(X_trn, y_trn, X_val, y_val, params=k, method="squared")
print(print_helper(["K", "TRAIN MSE", "TEST MSE"], MSE_error))

MAE_error = KNN_error(X_trn, y_trn, X_val, y_val, params=k, method="abs")
print(print_helper(["K", "TRAIN MAE", "TEST MAE"], MAE_error))

"""Question 3: Write a method to evaluate a linear regression model. where x is a 1D array with a single input, and w is a 1D array of regression coefficients of the same length. The return value y is just a scalar."""


def linear_reg_predict(x, w):
    # do stuff here
    y = 0
    if len(x) == len(w):
        return np.dot(np.array(x), np.array(w))
    else:
        print(
            "Dimensions of x and w do not match, cannot evaluate output of regression"
        )
        return


"""Question 4: Write a method to train a linear ridge regression model. """

import numpy as np


def linear_reg_train(X_trn, y_trn, l):
    X_trn_mat = np.array(X_trn)
    y_mat = np.array(y_trn)
    a = X_trn_mat.T.dot(X_trn_mat) + np.identity(len(X_trn[0])) * l
    b = X_trn_mat.T.dot(y_mat)
    w = np.linalg.solve(a, b)
    return w


"""Question 5 :  Repeat the evaluation process from question 2, except with different values of λ on the rows. Use the values λ=0, 0.001, 0.01, 0.1, 1, and 10. """
lambda_ = [0, 0.001, 0.01, 0.1, 1, 10]


def RIDGE_predict(X_trn, y_trn, X_val, y_val, lam, method):
    w = linear_reg_train(X_trn, y_trn, lam)
    output = [np.dot(w.T, x) for x in X_val]
    loss = error_cal(output, y_val, method)
    return loss


def RIDGE_error(X_trn, y_trn, X_val, y_val, lam, method="abs"):
    errors = []
    for l in lam:
        train_error = RIDGE_predict(X_trn, y_trn, X_trn, y_trn, l, method)
        test_error = RIDGE_predict(X_trn, y_trn, X_val, y_val, l, method)
        errors.append([l, round(train_error, 5), round(test_error, 5)])
    return errors


print("Q5:")

MSE_error = RIDGE_error(X_trn, y_trn, X_val, y_val, lam=lambda_, method="squared")
print(print_helper(["LAMBDA", "TRAIN MSE", "TEST MSE"], MSE_error))

MAE_error = RIDGE_error(X_trn, y_trn, X_val, y_val, lam=lambda_, method="abs")
print(print_helper(["LAMBDA", "TRAIN MAE", "TEST MAE"], MAE_error))

"""Question 6
1. Squared training error - A)Error only increases (or stays constant)

2. Squared test error - A)Error only increases (or stays constant)

3. Absolute training error - A)Error only increases (or stays constant)

4. Absolute test error - A)Error only increases (or stays constant)
"""
import matplotlib.pyplot as plt


def plot_error(x, y, title):
    plt.plot(x, y, marker="*")
    plt.title(title)
    plt.xlabel("Lambda")
    plt.ylabel(title[9:])
    plt.show()


x = [i[0] for i in MSE_error]
y = [i[1] for i in MSE_error]
plot_error(x, y, title="Lambda vs Mean Squared Training Error")
y = [i[2] for i in MSE_error]
plot_error(x, y, title="Lambda vs Mean Squared Test Error")
y = [i[1] for i in MAE_error]
plot_error(x, y, title="Lambda vs Mean Absolute Training Error")
y = [i[2] for i in MAE_error]
plot_error(x, y, title="Lambda vs Mean Absolute Test Error")

"""Question 7: Write a method to evaluate a regression stump."""


def reg_stump_predict(x, dim, thresh, c_left, c_right):
    # do stuff here
    if x[dim] <= thresh:
        return c_left
    else:
        return c_right


"""Question 8: Write a method to train a regression stump. where `X_trn` is a 2D array of training inputs, and `y_trn` is a 1D array of training outputs. The values `dim`, `thresh`, `c_left`, and `c_right` are as in the prediction function above."""


def reg_stump_train(X_trn, y_trn):
    # do stuff here
    err = {}
    for i, val in enumerate(X_trn):
        z_array = sorted(val)
        for j in range(1, len(z_array)):
            r1 = []
            r2 = []
            s = 0.5 * (z_array[j] + z_array[j - 1])
            for n, x_n in enumerate(val):
                if x_n < s:
                    r1.append(y_trn[n])
                else:
                    r2.append(y_trn[n])
            c_left = sum(r1) / len(r1)
            c_right = sum(r2) / len(r2)
            error = sum([(k - c_left) ** 2 for k in r1]) + sum(
                [(l - c_right) ** 2 for l in r2]
            )
            err[error] = (i, s, c_left, c_right)

    min_error = min(err.keys())
    return err[min_error]


"""Question 9: For the same data as above, train and evaluate a regression stump using your code from the previous two questions. Give a table with the training squared error, the test squared error, the training absolute error, and the test absolute error."""

dim, thresh, c_left, c_right = reg_stump_train(X_trn.T, y_trn)
print("Q9:")


def REG_STUMP_predict(X_trn, y_trn, X_val, y_val, method):
    preds = [reg_stump_predict(x, dim, thresh, c_left, c_right) for x in X_val]
    return error_cal(preds, y_val, method)


def REG_STUMP_error(X_trn, y_trn, X_val, y_val, method):
    train_error = REG_STUMP_predict(X_trn, y_trn, X_trn, y_trn, method)
    test_error = REG_STUMP_predict(X_trn, y_trn, X_val, y_val, method)
    return [[round(train_error, 5), round(test_error, 5)]]


MSE_error = REG_STUMP_error(X_trn, y_trn, X_val, y_val, method="squared")
print(print_helper(["TRAIN MSE", "TEST MSE"], MSE_error))

MAE_error = REG_STUMP_error(X_trn, y_trn, X_val, y_val, method="abs")
print(print_helper(["TRAIN MAE", "TEST MAE"], MAE_error))


"""Question 10:  K-nearest neighbors regression, with the following values of K: 1, 2, 5, 10, 20, 50"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
from numpy import load

data = load("big_data.npz")
X_trn_big = data["X_trn"]
y_trn_big = data["y_trn"]
X_val_big = data["X_val"]
y_val_big = data["y_val"]

headers = ["LAMBDA", "TRAIN MSE", "TEST MSE"]
rows = []
for k in [1, 2, 5, 10, 20, 50]:
    model = KNeighborsRegressor(n_neighbors=k)
    model.fit(X_trn_big, y_trn_big)
    pred_val_y = model.predict(X_val_big)
    val_mse = mean_squared_error(y_val_big, pred_val_y)

    pred_train_y = model.predict(X_trn_big)
    train_mse = mean_squared_error(y_trn_big, pred_train_y)
    rows.append([int(k), round(train_mse, 5), round(val_mse, 5)])
    # print(f"k = {k}, Train MSE = {train_mse}, Test MSE = {val_mse}")

print("Q 10:\nK NEAREST NEIGHBORS")
print(print_helper(headers, rows))

"""Question 11: Regression trees trained to (greedily) minimize squared error. Grow regression trees to the following maximum depths: 1,2,3,4,5."""

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

depths = [1, 2, 3, 4, 5]
headers = ["DEPTH", "TRAIN MSE", "TEST MSE"]
rows = []
for d in depths:
    model = DecisionTreeRegressor(random_state=0, max_depth=d)
    model.fit(X_trn_big, y_trn_big)
    pred_val_y = model.predict(X_val_big)
    val_mse = mean_squared_error(y_val_big, pred_val_y)

    pred_train_y = model.predict(X_trn_big)
    train_mse = mean_squared_error(y_trn_big, pred_train_y)
    rows.append([int(d), round(train_mse, 5), round(val_mse, 5)])
    # print(f"depth = {d}, Train MSE = {train_mse}, Test MSE = {val_mse}")

print("Q 11:\nK REGRESSION TREE")
print(print_helper(headers, rows))

"""Question 12: Ridge regression with each of the following regularization constants λ: 0, 1, 10, 100, 1000, 10000."""

from sklearn.linear_model import Ridge

reg_constants = [0, 1, 10, 100, 1000, 10000]

headers = ["LAMBDA", "TRAIN MSE", "TEST MSE"]
rows = []

for r in reg_constants:
    model = Ridge(alpha=r)
    model.fit(X_trn_big, y_trn_big)
    pred_val_y = model.predict(X_val_big)
    val_mse = mean_squared_error(y_val_big, pred_val_y)

    pred_train_y = model.predict(X_trn_big)
    train_mse = mean_squared_error(y_trn_big, pred_train_y)
    rows.append([int(r), round(train_mse, 5), round(val_mse, 5)])
    # print(f"Regularization Constant = {r}, Train MSE = {train_mse}, Test MSE = {val_mse}")

print("Q 12:\nK RIDGE REGRESSION")
print(print_helper(headers, rows))

"""Question 13:  Lasso regression with each of the following regularization constants λ: 0, .1, 1, 10, 100, 1000.

"""

from sklearn.linear_model import Lasso


headers = ["LAMBDA", "TRAIN MSE", "TEST MSE"]
rows = []
reg_constants = [0, 1, 10, 100, 1000, 10000]
# the reg constants as per the point noted will be 2N * lambda = alpha (in sklearn). alpha = (lambda)/2*N
alpha_constants = [lam / (2 * len(X_trn_big)) for lam in reg_constants]
for index, r in enumerate(alpha_constants):
    model = Lasso(alpha=r)
    model.fit(X_trn_big, y_trn_big)
    pred_val_y = model.predict(X_val_big)
    val_mse = mean_squared_error(y_val_big, pred_val_y)

    pred_train_y = model.predict(X_trn_big)
    train_mse = mean_squared_error(y_trn_big, pred_train_y)
    rows.append([int(reg_constants[index]), round(train_mse, 5), round(val_mse, 5)])
    # print(f"Regularization Constant = {reg_constants[index]}, Train MSE = {train_mse}, Test MSE = {val_mse}")

print("Q 13:\nK LASSO REGRESSION")
print(print_helper(headers, rows))
